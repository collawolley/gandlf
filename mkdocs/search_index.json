{
    "docs": [
        {
            "location": "/", 
            "text": "Generative Adversarial Network Deep Learning Framework\n\n\n\n\nHome is now behind you, the world is ahead!\n\n\n\n\nThis framework was built to make it possible for anyone to train \nGenerative Adversarial Networks\n. It is built on top of \nKeras\n.\n\n\nBecause it's built on top of Keras, it has the benefits of being \nmodular\n, \nminimal\n and \nextensible\n, running on both CPU and GPU using either Tensorflow or Theano.\n\n\nInstallation\n\n\nInstalling PyPi version:\n\n\npip install gandlf\n\n\n\n\nInstalling up-to-date version:\n\n\npip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models\n\n\n\n\nInstalling from source:\n\n\ngit clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install\n\n\n\n\nQuick Start\n\n\nBelow demonstrates how a Gandlf model works.\n\n\nimport keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n# \ninput_data\n represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -\n 1\n# and fake 'generated data' -\n 0, while the generator learns\n# 'generated data' -\n 1.\nmodel.fit(['normal', \ninput_data\n], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal', \ninput_data\n], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([\ninput_data\n])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')\n\n\n\n\nGuiding Principles\n\n\nIn no particular order:\n\n\n\n\nKeras-esque\n: The APIs should feel familiar for Keras users, with some minor changes.\n\n\nPowerful\n: Models should support a wide variety of GAN architectures.\n\n\nExtensible\n: Models should be easy to modify for different experiments.\n\n\n\n\nIssues Etiquette\n\n\nMore examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the \nexamples\n directory. Just create a pull request for it.\n\n\nContribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).\n\n\nIf you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Home"
        }, 
        {
            "location": "/#generative-adversarial-network-deep-learning-framework", 
            "text": "Home is now behind you, the world is ahead!   This framework was built to make it possible for anyone to train  Generative Adversarial Networks . It is built on top of  Keras .  Because it's built on top of Keras, it has the benefits of being  modular ,  minimal  and  extensible , running on both CPU and GPU using either Tensorflow or Theano.", 
            "title": "Generative Adversarial Network Deep Learning Framework"
        }, 
        {
            "location": "/#installation", 
            "text": "Installing PyPi version:  pip install gandlf  Installing up-to-date version:  pip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models  Installing from source:  git clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/#quick-start", 
            "text": "Below demonstrates how a Gandlf model works.  import keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n#  input_data  represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -  1\n# and fake 'generated data' -  0, while the generator learns\n# 'generated data' -  1.\nmodel.fit(['normal',  input_data ], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal',  input_data ], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([ input_data ])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#guiding-principles", 
            "text": "In no particular order:   Keras-esque : The APIs should feel familiar for Keras users, with some minor changes.  Powerful : Models should support a wide variety of GAN architectures.  Extensible : Models should be easy to modify for different experiments.", 
            "title": "Guiding Principles"
        }, 
        {
            "location": "/#issues-etiquette", 
            "text": "More examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the  examples  directory. Just create a pull request for it.  Contribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).  If you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Issues Etiquette"
        }, 
        {
            "location": "/background/", 
            "text": "What is a GAN?\n\n\nGAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.\n\n\n\n\nThe GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.\n\n\nGANs were introduced by Ian Goodfellow in an \neponymous paper\n. Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular \nKeras\n library, and therefore supports training in both Theano and Tensorflow.\n\n\nWhy are GANs interesting?\n\n\nGANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).\n\n\nTo understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.\n\n\nWhat is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.\n\n\nResources\n\n\nBelow are some resources for learning more about GANs.\n\n\n\n\n/r/MachineLearning\n\n\nAdversarial Training Facebook Group", 
            "title": "Background"
        }, 
        {
            "location": "/background/#what-is-a-gan", 
            "text": "GAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.   The GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.  GANs were introduced by Ian Goodfellow in an  eponymous paper . Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular  Keras  library, and therefore supports training in both Theano and Tensorflow.", 
            "title": "What is a GAN?"
        }, 
        {
            "location": "/background/#why-are-gans-interesting", 
            "text": "GANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).  To understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.  What is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.", 
            "title": "Why are GANs interesting?"
        }, 
        {
            "location": "/background/#resources", 
            "text": "Below are some resources for learning more about GANs.   /r/MachineLearning  Adversarial Training Facebook Group", 
            "title": "Resources"
        }, 
        {
            "location": "/model/", 
            "text": "Gandlf models employ a few tricks to make training Generative Adversarial Networks much easier than they would be in conventional Keras code.\n\n\nInputs\n\n\n\n\nThe principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...\n\n\n\n\nConceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:\n\n\nlatent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal', \nreal_data\n], ...)\n\n\n\n\nBy specifying \nnormal\n as the input to \nlatent_vec\n, the model will pull random data from a normal distribution to populate the \nlatent_vec\n input whenever it is needed. Similarly, \nuniform\n can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument \nbatch_size\n and returns the desired Numpy array:\n\n\nimport numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func, \nreal_data\n], ...)\n\n\n\n\nModes\n\n\nGenerative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards \nfake\n and to push the real data towards \nreal\n.\n\n\nA typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:\n\n\nis_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)\n\n\n\n\nOne problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:\n\n\n\n\nThe suffixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the \nfit\n function on the model above:\n\n\nmodel.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen': \ncorrect_classes\n\n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake': \nsome_classes\n\n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real': \ncorrect_classes\n\n})\n\n\n\n\nThe \nfit\n function can be written slightly more compactly by combining the \ngen\n and \nreal\n target classes. This is written as:\n\n\nmodel.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen': \ncorrect_classes\n, 'aux_class_fake': \nsome_classes\n,\n})\n\n\n\n\nSimilar shorthand can be used when specifying \noptimizer\n and \nloss\n in the \ncompile\n method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:\n\n\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})", 
            "title": "Model"
        }, 
        {
            "location": "/model/#inputs", 
            "text": "The principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...   Conceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:  latent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal',  real_data ], ...)  By specifying  normal  as the input to  latent_vec , the model will pull random data from a normal distribution to populate the  latent_vec  input whenever it is needed. Similarly,  uniform  can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument  batch_size  and returns the desired Numpy array:  import numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func,  real_data ], ...)", 
            "title": "Inputs"
        }, 
        {
            "location": "/model/#modes", 
            "text": "Generative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards  fake  and to push the real data towards  real .  A typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:  is_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)  One problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:   The suffixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the  fit  function on the model above:  model.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen':  correct_classes \n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake':  some_classes \n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real':  correct_classes \n})  The  fit  function can be written slightly more compactly by combining the  gen  and  real  target classes. This is written as:  model.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen':  correct_classes , 'aux_class_fake':  some_classes ,\n})  Similar shorthand can be used when specifying  optimizer  and  loss  in the  compile  method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:  model.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})", 
            "title": "Modes"
        }, 
        {
            "location": "/tricks/", 
            "text": "General\n\n\nA list of useful GAN hacks is available \nhere\n. Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.\n\n\nData\n\n\n\n\nNormalize the data to \n[-1, 1]\n\n\nThis data can then be approximated using the \ntanh\n function\n\n\n\n\n\n\nUse Batch Normalization (implemented in Keras as \nBatchNormalization\n)\n\n\n\n\nModel\n\n\n\n\nUse \nkeras.layers.LeakyReLU\n instead of the ReLU activation function\n\n\nIf available, use labeled data (Auxiliary Classifier GAN or Conditional GAN). These can be integrated as inputs and outputs or as \nattention\n components.\n\n\nUse dropout in both the train and test phase: This is implemented as \ngandlf.layers.PermanentDropout\n\n\nUse minibatch discrimination: Compare the intrabatch similarities and add add it as a feature for the discriminator. This helps the model avoid generating only type of output. This is implemented as \ngandlf.layers.BatchSimilarity\n.\n\n\n\n\nTraining\n\n\n\n\nModified loss function: Instead of training the Generator to minimize \nlog(1-D)\n, train it to minimize \n-log(D)\n. This is implemented as \ngandlf.losses.negative_binary_crossentropy\n\n\nFor the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as \nmodel.fit(inputs=['normal', ...)\n\n\nReinforcement learning stability tricks: Not yet implemented\n\n\nUse the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as \nmodel = gandlf.Model(optimizer=['adam', 'sgd'], ...)\n\n\nDon't let the discriminator saturate!\n\n\nAdapt the generator and discriminator updates so that when the generator loss is high relative to the discriminator, its learning rate is also higher. This is implemented as \ngandlf.callbacks.AdaptiveLearningRate", 
            "title": "Tricks"
        }, 
        {
            "location": "/tricks/#general", 
            "text": "A list of useful GAN hacks is available  here . Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.", 
            "title": "General"
        }, 
        {
            "location": "/tricks/#data", 
            "text": "Normalize the data to  [-1, 1]  This data can then be approximated using the  tanh  function    Use Batch Normalization (implemented in Keras as  BatchNormalization )", 
            "title": "Data"
        }, 
        {
            "location": "/tricks/#model", 
            "text": "Use  keras.layers.LeakyReLU  instead of the ReLU activation function  If available, use labeled data (Auxiliary Classifier GAN or Conditional GAN). These can be integrated as inputs and outputs or as  attention  components.  Use dropout in both the train and test phase: This is implemented as  gandlf.layers.PermanentDropout  Use minibatch discrimination: Compare the intrabatch similarities and add add it as a feature for the discriminator. This helps the model avoid generating only type of output. This is implemented as  gandlf.layers.BatchSimilarity .", 
            "title": "Model"
        }, 
        {
            "location": "/tricks/#training", 
            "text": "Modified loss function: Instead of training the Generator to minimize  log(1-D) , train it to minimize  -log(D) . This is implemented as  gandlf.losses.negative_binary_crossentropy  For the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as  model.fit(inputs=['normal', ...)  Reinforcement learning stability tricks: Not yet implemented  Use the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as  model = gandlf.Model(optimizer=['adam', 'sgd'], ...)  Don't let the discriminator saturate!  Adapt the generator and discriminator updates so that when the generator loss is high relative to the discriminator, its learning rate is also higher. This is implemented as  gandlf.callbacks.AdaptiveLearningRate", 
            "title": "Training"
        }, 
        {
            "location": "/layers/core/", 
            "text": "core.py\n\n\nCore layers, tricks which can help easily improve GANs in many cases.\n\n\nPermanentDropout\n\n\ngandlf.layers.PermanentDropout()\n\n\n\n\nAn alternative to Keras \nDropout\n which stays active during both training and testing.\n\n\nBatchSimilarity\n\n\ngandlf.layers.BatchSimilarity(similarity='exp_l1')\n\n\n\n\nCalculates the minibatch similarities, a trick introduced in \nImproved Techniques for Training GANs\n. These similarities can be added as features for the existing layer by using a Merge layer. The layer takes as input a 2D Tensor with shape \n(batch_size, num_features)\n and outputs a Tensor with shape \n(batch_size, num_similarities)\n, where \nnum_similarities\n is the total number of computed similarities.\n\n\nIn order to make this layer linear time with respect to the batch size, instead of doing a pairwise comparison between each pair of samples in the batch, for each sample a random sample is uniformly selected with which to do pairwise comparison.\n\n\nThe \nsimilarity\n argument can be one of:\n\n\n\n\nexp_l1\n: \nexp(sum(abs(a - b)))\n\n\nexp_l2\n or \nrbf\n: \nexp(sum(square(a - b)))\n\n\nl1\n: \nsum(abs(a - b))\n\n\nl2\n: \nsum(square(a - b))\n\n\ncosine\n: \ndot(a, b) / (|| a || * || b ||)\n\n\nsigmoid\n: \nsigmoid(dot(a, b))\n\n\neuclidean\n: \n1 / (1 + sum(square(a - b)))\n\n\ngeometric\n: \nsigmoid(a, b) * euclidean(a, b)\n\n\narithmetic\n: \n(sigmoid(a, b) + euclidean(a, b)) / 2\n\n\n\n\nThese implementations can be found in \nsimilarities.py\n.\n\n\nAlternatively, a function can be provided which take two tensors and returns their similarity. Multiple similarity arguments can be provided as a list or tuple. \n\n\nThe following example illustrates how to merge the similarity features with the layer output:\n\n\nsims = gandlf.layers.BatchSimilarity(['sim1', 'sim2', etc.])(input_layer)\noutput_layer = keras.layers.merge([input_layer, sims], mode='concat')", 
            "title": "Core"
        }, 
        {
            "location": "/layers/core/#permanentdropout", 
            "text": "gandlf.layers.PermanentDropout()  An alternative to Keras  Dropout  which stays active during both training and testing.", 
            "title": "PermanentDropout"
        }, 
        {
            "location": "/layers/core/#batchsimilarity", 
            "text": "gandlf.layers.BatchSimilarity(similarity='exp_l1')  Calculates the minibatch similarities, a trick introduced in  Improved Techniques for Training GANs . These similarities can be added as features for the existing layer by using a Merge layer. The layer takes as input a 2D Tensor with shape  (batch_size, num_features)  and outputs a Tensor with shape  (batch_size, num_similarities) , where  num_similarities  is the total number of computed similarities.  In order to make this layer linear time with respect to the batch size, instead of doing a pairwise comparison between each pair of samples in the batch, for each sample a random sample is uniformly selected with which to do pairwise comparison.  The  similarity  argument can be one of:   exp_l1 :  exp(sum(abs(a - b)))  exp_l2  or  rbf :  exp(sum(square(a - b)))  l1 :  sum(abs(a - b))  l2 :  sum(square(a - b))  cosine :  dot(a, b) / (|| a || * || b ||)  sigmoid :  sigmoid(dot(a, b))  euclidean :  1 / (1 + sum(square(a - b)))  geometric :  sigmoid(a, b) * euclidean(a, b)  arithmetic :  (sigmoid(a, b) + euclidean(a, b)) / 2   These implementations can be found in  similarities.py .  Alternatively, a function can be provided which take two tensors and returns their similarity. Multiple similarity arguments can be provided as a list or tuple.   The following example illustrates how to merge the similarity features with the layer output:  sims = gandlf.layers.BatchSimilarity(['sim1', 'sim2', etc.])(input_layer)\noutput_layer = keras.layers.merge([input_layer, sims], mode='concat')", 
            "title": "BatchSimilarity"
        }, 
        {
            "location": "/layers/attention/", 
            "text": "attention.py\n\n\nRecurrentAttention1D\n\n\ngandlf.layers.RecurrentAttention1D(layer, attention, attn_activation='tanh', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer)\n\n\n\n\nMakes the wrapped Keras RNN \nlayer\n pay attention to the \nattention\n tensor, which has shape \n(batch_size, num_attn_features)\n.\n\n\nThe updated hidden state is computed after each timestep as:\n\n\ntrainable_params = [U_m, b_m, U_a, U_s, b_s]\n\n# Given hidden output \nh\n at each timestep:\nm = attn_activation(dot(h, U_m) + dot(attention, U_a) + b_m)\ns = attn_gate(dot(m, U_s) + b_s)\nh_new = s * h  # Element-wise weighting.\n\n\n\n\nRecurrentAttention2D\n\n\ngandlf.layers.RecurrentAttention2D(layer, attention, time_dist_activation='softmax', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer=None)\n\n\n\n\nMakes the wrapped Keras RNN \nlayer\n pay attention to the \nattention\n tensor, which has shape \n(batch_size, num_attn_timesteps, num_attn_features)\n.\n\n\nThe updated hidden state is computed after each timestep as:\n\n\ntrainable_params = [U_t, b_t, U_a, b_a]\n\n# Given hidden output \nh\n at each timestep:\nt = time_dist_activation(dot(h, U_t), b_t)\nw = sum(t * attention)  # Weights each timestep by `t`.\ns = attn_gate_func(dot(w, U_a) + b_a)\nh_new = s * h  # Element-wise weighting.", 
            "title": "Attention"
        }, 
        {
            "location": "/layers/attention/#recurrentattention1d", 
            "text": "gandlf.layers.RecurrentAttention1D(layer, attention, attn_activation='tanh', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer)  Makes the wrapped Keras RNN  layer  pay attention to the  attention  tensor, which has shape  (batch_size, num_attn_features) .  The updated hidden state is computed after each timestep as:  trainable_params = [U_m, b_m, U_a, U_s, b_s]\n\n# Given hidden output  h  at each timestep:\nm = attn_activation(dot(h, U_m) + dot(attention, U_a) + b_m)\ns = attn_gate(dot(m, U_s) + b_s)\nh_new = s * h  # Element-wise weighting.", 
            "title": "RecurrentAttention1D"
        }, 
        {
            "location": "/layers/attention/#recurrentattention2d", 
            "text": "gandlf.layers.RecurrentAttention2D(layer, attention, time_dist_activation='softmax', attn_gate_func='sigmoid', W_regularizer=None, b_regularizer=None)  Makes the wrapped Keras RNN  layer  pay attention to the  attention  tensor, which has shape  (batch_size, num_attn_timesteps, num_attn_features) .  The updated hidden state is computed after each timestep as:  trainable_params = [U_t, b_t, U_a, b_a]\n\n# Given hidden output  h  at each timestep:\nt = time_dist_activation(dot(h, U_t), b_t)\nw = sum(t * attention)  # Weights each timestep by `t`.\ns = attn_gate_func(dot(w, U_a) + b_a)\nh_new = s * h  # Element-wise weighting.", 
            "title": "RecurrentAttention2D"
        }, 
        {
            "location": "/layers/wrappers/", 
            "text": "wrappers.py\n\n\nResidual\n\n\ngandlf.layers.Residual(layer, merge_mode='sum')\n\n\n\n\nApplies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.\n\n\nThe provided \nlayer\n has to have the same input and output dimensions. Given an input \nx\n, the output is:\n\n\noutput = merge_mode(x, layer(x))\n\n\n\n\nmerge_mode\n can be a string like for \nMerge\n or a \nMerge\n layer itself.", 
            "title": "Wrappers"
        }, 
        {
            "location": "/layers/wrappers/#residual", 
            "text": "gandlf.layers.Residual(layer, merge_mode='sum')  Applies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.  The provided  layer  has to have the same input and output dimensions. Given an input  x , the output is:  output = merge_mode(x, layer(x))  merge_mode  can be a string like for  Merge  or a  Merge  layer itself.", 
            "title": "Residual"
        }, 
        {
            "location": "/callbacks/", 
            "text": "callbacks.py\n\n\nAdaptiveLearningRate\n\n\ngandlf.callbacks.AdaptiveLearningRate(discriminator_lr, generator_lr)\n\n\n\n\nAdapts the learning rate on each batch according to the model's loss, where \ndiscriminator_lr\n and \ngenerator_lr\n are the maximum learning rates for the discriminator and generator models.\n\n\nOn each batch, the learning rate for the generator and discriminator are adapted according to the losses of each:\n\n\nbatch_dis_lr = discriminator_lr * (discriminator_loss / total_loss)\nbatch_gen_lr = generator_lr * (generator_loss / total_loss)", 
            "title": "Callbacks"
        }, 
        {
            "location": "/callbacks/#adaptivelearningrate", 
            "text": "gandlf.callbacks.AdaptiveLearningRate(discriminator_lr, generator_lr)  Adapts the learning rate on each batch according to the model's loss, where  discriminator_lr  and  generator_lr  are the maximum learning rates for the discriminator and generator models.  On each batch, the learning rate for the generator and discriminator are adapted according to the losses of each:  batch_dis_lr = discriminator_lr * (discriminator_loss / total_loss)\nbatch_gen_lr = generator_lr * (generator_loss / total_loss)", 
            "title": "AdaptiveLearningRate"
        }, 
        {
            "location": "/losses/", 
            "text": "losses.py\n\n\nNegative Binary Crossentropy\n\n\nInstead of minimizing \nlog(1-D)\n maximize \nlog(D)\n.\n\n\nNote that when using this loss function, you should not change the target. For example, if you want \nG -\n 0\n and \nD -\n 1\n to train your generator, you should replace your \nbinary_crossentropy\n for the fake output with \nnegative_binary_crossentropy\n while keeping the target output as 0.\n\n\nMaximize\n\n\nMaximizes \ny_true\n, regardless of \ny_pred\n.\n\n\nMinimize\n\n\nMinimizes \ny_true\n, regardless of \ny_pred\n.", 
            "title": "Losses"
        }, 
        {
            "location": "/losses/#negative-binary-crossentropy", 
            "text": "Instead of minimizing  log(1-D)  maximize  log(D) .  Note that when using this loss function, you should not change the target. For example, if you want  G -  0  and  D -  1  to train your generator, you should replace your  binary_crossentropy  for the fake output with  negative_binary_crossentropy  while keeping the target output as 0.", 
            "title": "Negative Binary Crossentropy"
        }, 
        {
            "location": "/losses/#maximize", 
            "text": "Maximizes  y_true , regardless of  y_pred .", 
            "title": "Maximize"
        }, 
        {
            "location": "/losses/#minimize", 
            "text": "Minimizes  y_true , regardless of  y_pred .", 
            "title": "Minimize"
        }, 
        {
            "location": "/examples/xor/", 
            "text": "xor.py\n\n\nThis example can be run quickly on a CPU, and is a good demonstration of one of the tricky parts about training GANs. The input data consists of four uniform distributions, centered near \n(-1, -1)\n, \n(1, -1)\n, \n(-1, 1)\n and \n(1, 1)\n. This is illustrated in the figure below, which each of the distributions labeled.\n\n\n\n\nThe model can either be trained in unsupervised mode or supervised mode. In the supervised mode, it acts as an auxiliary classifier GAN, which explicitly says which distribution the generated data should come from.\n\n\nWhen trained in the unsupervised mode, the data tends to cluster in one of the distributions. A potential way to fix this would be to let the GAN look at a whole batch of data, which would let it know that it's clustering too much on one distribution.", 
            "title": "XOR"
        }, 
        {
            "location": "/examples/mnist_gan/", 
            "text": "mnist_gan.py\n\n\nThis example illustrates how to use a Gandlf model to generate MNIST digits. The model can be run in supervised mode (where the discriminator and generator know the desired class labels) or unsupervised mode (which is a more pure GAN implementation).\n\n\nThis example is a Gandlf implementation of the Keras MNIST ACGAN example, which can be found \nhere\n. One important distinction is that Gandlf runs the generator and discriminator updates in parallel rather than sequentially; this can't be done in Keras normally.\n\n\nTODO: Add images sampled from this model.\n\n\nIn addition to the convolutional model, a simple feed-forward model can be used for the discriminator and generator which can be trained much more quickly (feasible to run it on a laptop). The samples below show randomly sampled generated images from the lite model, which took about 100 seconds per epoch on a Macbook Pro.\n\n\n\n\nThe gif below shows the representation of a 6 when the model is fed latent vectors that interpolate between two points. Because the lite model is not convolutional, the interpolation is not very smoothe.", 
            "title": "MNIST GAN"
        }, 
        {
            "location": "/examples/mnist_rnn_gan/", 
            "text": "mnist_rnn_gan.py\n\n\nThis example illustrates:\n\n\n\n\nUsing RNNs in your GAN architecture\n\n\nUsing attention components to direct GAN learning (with the recurrent attention wrappers, \ngandlf.layers.RecurrentAttention1D\n and \ngandlf.layers.RecurrentAttention2D\n)", 
            "title": "MNIST RNN GAN"
        }, 
        {
            "location": "/examples/vgg16/", 
            "text": "vgg16.py\n\n\nThis example illustrates how a pre-trained model can be used to generate images. The discriminator is the VGG16 model which is available with Keras, and the discriminator weights are frozen. The generator learns to generate samples of a desired class. This isn't a pure GAN, because the discriminator model doesn't learn to discriminate adversarial examples, but the generator does learn to map a random normal distribution to generate image classes in the ImageNet dataset.\n\n\nTODO: Add images sampled from this model (it is really big and takes a while to train, so they will be added whenever I have access to a good enough computer).", 
            "title": "VGG16"
        }
    ]
}