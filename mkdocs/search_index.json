{
    "docs": [
        {
            "location": "/", 
            "text": "Generative Adversarial Network Deep Learning Framework\n\n\n\n\nHe that breaks a thing to find out what it is has left the path of wisdom.\n(Tim Peters, Council of Elrond Style Guide)\n\n\n\n\nThis is a framework built on top of \nKeras\n for training \nGenerative Adversarial Networks\n.\n\n\nBecause it's built on top of Keras, it has the benefits of being \nmodular\n, \nminimal\n and \nextensible\n, running on both CPU and GPU using either Tensorflow or Theano.\n\n\nInstallation\n\n\nUsing Pip:\n\n\npip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models\n\n\n\n\nInstalling from source:\n\n\ngit clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install\n\n\n\n\nQuick Start\n\n\nBelow demonstrates how a Gandlf model works.\n\n\nimport keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n# \ninput_data\n represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -\n 1\n# and fake 'generated data' -\n 0, while the generator learns\n# 'generated data' -\n 1.\nmodel.fit(['normal', \ninput_data\n], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal', \ninput_data\n], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([\ninput_data\n])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')\n\n\n\n\nGuiding Principles\n\n\nIn no particular order:\n\n\n\n\nKeras-esque\n: The APIs should feel familiar for Keras users, with some minor changes.\n\n\nPowerful\n: Models should support a wide variety of GAN architectures.\n\n\nExtensible\n: Models should be easy to modify for different experiments.\n\n\n\n\nIssues Etiquette\n\n\nMore examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the \nexamples\n directory. Just create a pull request for it.\n\n\nContribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).\n\n\nIf you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Home"
        }, 
        {
            "location": "/#generative-adversarial-network-deep-learning-framework", 
            "text": "He that breaks a thing to find out what it is has left the path of wisdom.\n(Tim Peters, Council of Elrond Style Guide)   This is a framework built on top of  Keras  for training  Generative Adversarial Networks .  Because it's built on top of Keras, it has the benefits of being  modular ,  minimal  and  extensible , running on both CPU and GPU using either Tensorflow or Theano.", 
            "title": "Generative Adversarial Network Deep Learning Framework"
        }, 
        {
            "location": "/#installation", 
            "text": "Using Pip:  pip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models  Installing from source:  git clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/#quick-start", 
            "text": "Below demonstrates how a Gandlf model works.  import keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n#  input_data  represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -  1\n# and fake 'generated data' -  0, while the generator learns\n# 'generated data' -  1.\nmodel.fit(['normal',  input_data ], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal',  input_data ], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src_gen': '1', 'src_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([ input_data ])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\nmodel.generator.save('/generator/save/path')\nmodel.discriminator.save('/discriminator/save/path')", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#guiding-principles", 
            "text": "In no particular order:   Keras-esque : The APIs should feel familiar for Keras users, with some minor changes.  Powerful : Models should support a wide variety of GAN architectures.  Extensible : Models should be easy to modify for different experiments.", 
            "title": "Guiding Principles"
        }, 
        {
            "location": "/#issues-etiquette", 
            "text": "More examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the  examples  directory. Just create a pull request for it.  Contribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).  If you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Issues Etiquette"
        }, 
        {
            "location": "/background/", 
            "text": "What is a GAN?\n\n\nGAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.\n\n\n\n\nThe GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.\n\n\nGANs were introduced by Ian Goodfellow in an \neponymous paper\n. Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular \nKeras\n library, and therefore supports training in both Theano and Tensorflow.\n\n\nWhy are GANs interesting?\n\n\nGANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).\n\n\nTo understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.\n\n\nWhat is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.\n\n\nResources\n\n\nBelow are some resources for learning more about GANs.\n\n\n\n\n/r/MachineLearning\n\n\nAdversarial Training Facebook Group", 
            "title": "Background"
        }, 
        {
            "location": "/background/#what-is-a-gan", 
            "text": "GAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.   The GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.  GANs were introduced by Ian Goodfellow in an  eponymous paper . Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular  Keras  library, and therefore supports training in both Theano and Tensorflow.", 
            "title": "What is a GAN?"
        }, 
        {
            "location": "/background/#why-are-gans-interesting", 
            "text": "GANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).  To understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.  What is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.", 
            "title": "Why are GANs interesting?"
        }, 
        {
            "location": "/background/#resources", 
            "text": "Below are some resources for learning more about GANs.   /r/MachineLearning  Adversarial Training Facebook Group", 
            "title": "Resources"
        }, 
        {
            "location": "/model/", 
            "text": "Gandlf models employ a few tricks to make training Generative Adversarial Networks much easier than they would be in conventional Keras code.\n\n\nInputs\n\n\n\n\nThe principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...\n\n\n\n\nConceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:\n\n\nlatent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal', \nreal_data\n], ...)\n\n\n\n\nBy specifying \nnormal\n as the input to \nlatent_vec\n, the model will pull random data from a normal distribution to populate the \nlatent_vec\n input whenever it is needed. Similarly, \nuniform\n can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument \nbatch_size\n and returns the desired Numpy array:\n\n\nimport numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func, \nreal_data\n], ...)\n\n\n\n\nModes\n\n\nGenerative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards \nfake\n and to push the real data towards \nreal\n.\n\n\nA typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:\n\n\nis_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)\n\n\n\n\nOne problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:\n\n\n\n\nThe prefixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the \nfit\n function on the model above:\n\n\nmodel.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen': \ncorrect_classes\n\n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake': \nsome_classes\n\n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real': \ncorrect_classes\n\n})\n\n\n\n\nThe \nfit\n function can be written slightly more compactly by combining the \ngen\n and \nreal\n target classes. This is written as:\n\n\nmodel.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen': \ncorrect_classes\n, 'aux_class_fake': \nsome_classes\n,\n})\n\n\n\n\nSimilar shorthand can be used when specifying \noptimizer\n and \nloss\n in the \ncompile\n method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:\n\n\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})", 
            "title": "Model"
        }, 
        {
            "location": "/model/#inputs", 
            "text": "The principle of generating small amounts of finite improbability by simply hooking the logic circuits of a Bambleweeny 57 Sub-Meson Brain to an atomic vector plotter suspended in a strong Brownian Motion producer (say a nice hot cup of tea) were of course well understood...   Conceptually, the Generator part of a Generative Adversarial Network learns to map a random data distribution to the distribution of real data. When building them, you therefore need a good way to produce random latent variables. In Gandlf, this can be done easily, as follows:  latent_vec = keras.layers.Input(shape, name='latent_vec')\ngenerator_model = keras.models.Model(input=[latent_vec], ...)\nmodel = gandlf.Model(generator=generator_model, discriminator=...)\nmodel.compile(optimizer, loss)\nmodel.fit(x=['normal',  real_data ], ...)  By specifying  normal  as the input to  latent_vec , the model will pull random data from a normal distribution to populate the  latent_vec  input whenever it is needed. Similarly,  uniform  can be used for the same purpose. By default, the distribution has a mean of zero and variance of one. To change this, a function can be passed which takes as an argument  batch_size  and returns the desired Numpy array:  import numpy as np\nfunc = lambda batch_size: np.random.normal(loc=1., scale=0.1, size=(batch_size, 3, 4))\nmodel.fit(x=[func,  real_data ], ...)", 
            "title": "Inputs"
        }, 
        {
            "location": "/model/#modes", 
            "text": "Generative adversarial networks have two modes: the generator part and the discriminator part. In Gandlf, the discriminator part is also separated into two modes; learning to push the generated data towards  fake  and to push the real data towards  real .  A typical discriminator model might have two outputs: one to predict if the sample being recieved is real or fake data, and one to act as an auxiliary classifier (in the example of MNIST digits, the auxiliary classifier might predict which digit the sample belongs to). This can be done in Gandlf as follows:  is_it_real = keras.layers.SomeLayer(..., name='is_it_real')\naux_class = keras.layers.SomeLayer(..., name='aux_class')\ndiscriminator = Model(inputs=[...], outputs=[is_it_real, aux_class])\nmodel = gandlf.Model(generator=..., discriminator=discriminator)  One problem is that the outputs have to be specified for each mode. The diagram below illustrates the three training modes:   The prefixes above are used in Gandlf naming to specify the outputs. The example below illustrates how to specify the desired outputs for the  fit  function on the model above:  model.fit(inputs=[...], outputs={\n    # Tells the generator to make the discriminator's is_it_real output\n    # go towards 1 for generated samples.\n    'is_it_real_gen': 'ones',\n\n    # Tells the generator to generate samples that are classified as\n    # their correct classification by the discriminator.\n    'aux_class_gen':  correct_classes \n\n    # Tells the discriminator to make the is_it_real output go towards 0\n    # for generated samples.\n    'is_it_real_fake': 'zeros',\n\n    # Tells the discriminator to classify the fake samples as\n    # some class (this can be turned off using loss_weights).\n    'aux_class_fake':  some_classes \n\n    # Tells the discriminator to make the is_it_real output go towards 1\n    # for real samples.\n    'is_it_real_real': 'ones',\n\n    # Tells the discriminator to classify the real samples as their\n    # correct classes.\n    'aux_class_real':  correct_classes \n})  The  fit  function can be written slightly more compactly by combining the  gen  and  real  target classes. This is written as:  model.fit(inputs=[...], outputs={\n    'is_it_real_gen_real': 'ones', 'is_it_real_fake': 'zeros',\n    'aux_class_real_gen':  correct_classes , 'aux_class_fake':  some_classes ,\n})  Similar shorthand can be used when specifying  optimizer  and  loss  in the  compile  method. For the optimizer, passing a list or tuple of optimizers with length 2 will assign the first optimizer to train the discriminator and the second optimizer to train the generator. For the loss, the same naming conventions apply:  model.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real': 'binary_crossentropy',\n    'aux_class': 'categorical_crossentropy',\n})\n\n# This method is equivalent, but much longer and more redundant.\nmodel.compile(optimizer=['adam', 'sgd'], loss={\n    'is_it_real_gen': 'binary_crossentropy',\n    'is_it_real_real': 'binary_crossentropy',\n    'is_it_real_fake': 'binary_crossentropy',\n\n    'aux_class_gen_real': 'categorical_crossentropy',\n    'aux_class_fake': 'categorical_crossentropy',\n})", 
            "title": "Modes"
        }, 
        {
            "location": "/tricks/", 
            "text": "General\n\n\nA list of useful GAN hacks is available \nhere\n. Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.\n\n\nData\n\n\n\n\nNormalize the data to \n[-1, 1]\n\n\nThis data can then be approximated using the \ntanh\n function\n\n\n\n\n\n\n\n\nModel\n\n\n\n\nUse \nkeras.layers.LeakyReLU\n instead of the ReLU activation function\n\n\nIf available, use labeled data (Auxiliary Classifier GAN or Conditional GAN)\n\n\nUse dropout in both the train and test phase: This is implemented as \ngandlf.layers.PermanentDropout\n\n\n\n\nTraining\n\n\n\n\nModified loss function: Instead of training the Generator to minimize \nlog(1-D)\n, train it to minimize \n-log(D)\n. This is implemented as \ngandlf.losses.negative_binary_crossentropy\n\n\nFor the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as \nmodel.fit(inputs=['normal', ...)\n\n\nReinforcement learning stability tricks: Not yet implemented\n\n\nUse the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as \nmodel = gandlf.Model(optimizer=['adam', 'sgd'], ...)\n\n\nDon't let the discriminator saturate!", 
            "title": "Tricks"
        }, 
        {
            "location": "/tricks/#general", 
            "text": "A list of useful GAN hacks is available  here . Most of the tricks listed below are taken from that list, with references for how to implement them in Gandlf.", 
            "title": "General"
        }, 
        {
            "location": "/tricks/#data", 
            "text": "Normalize the data to  [-1, 1]  This data can then be approximated using the  tanh  function", 
            "title": "Data"
        }, 
        {
            "location": "/tricks/#model", 
            "text": "Use  keras.layers.LeakyReLU  instead of the ReLU activation function  If available, use labeled data (Auxiliary Classifier GAN or Conditional GAN)  Use dropout in both the train and test phase: This is implemented as  gandlf.layers.PermanentDropout", 
            "title": "Model"
        }, 
        {
            "location": "/tricks/#training", 
            "text": "Modified loss function: Instead of training the Generator to minimize  log(1-D) , train it to minimize  -log(D) . This is implemented as  gandlf.losses.negative_binary_crossentropy  For the latent vector(s), sample from a normal distribution instead of a uniform distribution. This is implemented as  model.fit(inputs=['normal', ...)  Reinforcement learning stability tricks: Not yet implemented  Use the Adam optimizer for the discriminator, and the SGD optimizer for the generator. This can be done as  model = gandlf.Model(optimizer=['adam', 'sgd'], ...)  Don't let the discriminator saturate!", 
            "title": "Training"
        }, 
        {
            "location": "/layers/core/", 
            "text": "Core layers, tricks which can help easily improve GANs in many cases.\n\n\nPermanentDropout\n\n\nAn alternative to Keras Dropout which stays active during both training and testing.", 
            "title": "Core"
        }, 
        {
            "location": "/layers/core/#permanentdropout", 
            "text": "An alternative to Keras Dropout which stays active during both training and testing.", 
            "title": "PermanentDropout"
        }, 
        {
            "location": "/layers/attention/", 
            "text": "Attention mechanisms, which can help integrate more information into training GANs.\n\n\nRecurrentAttention1D\n\n\nAttention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape \n(batch_size, num_features)\n.\n\n\nRecurrentAttention2D\n\n\nAttention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape \n(batch_size, num_timesteps, num_features)\n.", 
            "title": "Attention"
        }, 
        {
            "location": "/layers/attention/#recurrentattention1d", 
            "text": "Attention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape  (batch_size, num_features) .", 
            "title": "RecurrentAttention1D"
        }, 
        {
            "location": "/layers/attention/#recurrentattention2d", 
            "text": "Attention mechanism for Recurrent Keras layers (VanillaRNN, GRU and LSTM) which pays attention to a tensor with shape  (batch_size, num_timesteps, num_features) .", 
            "title": "RecurrentAttention2D"
        }, 
        {
            "location": "/layers/wrappers/", 
            "text": "Residual\n\n\nApplies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.", 
            "title": "Wrappers"
        }, 
        {
            "location": "/layers/wrappers/#residual", 
            "text": "Applies a residual to any Keras layer or model, so long as it's inputs are the same dimension as its outputs. Useful for implementing residual architectures.", 
            "title": "Residual"
        }, 
        {
            "location": "/examples/xor/", 
            "text": "This example can be run quickly on a CPU, and is a good demonstration of one of the tricky parts about training GANs. The input data consists of four uniform distributions, centered near \n(-1, -1)\n, \n(1, -1)\n, \n(-1, 1)\n and \n(1, 1)\n. This is illustrated in the figure below, which each of the distributions labeled.\n\n\n\n\nThe model can either be trained in unsupervised mode or supervised mode. In the supervised mode, it acts as an auxiliary classifier GAN, which explicitly says which distribution the generated data should come from.\n\n\nWhen trained in the unsupervised mode, the data tends to cluster in one of the distributions. A potential way to fix this would be to let the GAN look at a whole batch of data, which would let it know that it's clustering too much on one distribution.", 
            "title": "XOR"
        }
    ]
}