{
    "docs": [
        {
            "location": "/", 
            "text": "Generative Adversarial Network Deep Learning Framework\n\n\n\n\nHe that breaks a thing to find out what it is has left the path of wisdom.\n(Tim Peters, Council of Elrond Style Guide)\n\n\n\n\nThis is a framework built on top of \nKeras\n for training \nGenerative Adversarial Networks\n.\n\n\nBecause it's built on top of Keras, it has the benefits of being \nmodular\n, \nminimal\n and \nextensible\n, running on both CPU and GPU using either Tensorflow or Theano.\n\n\nInstallation\n\n\nUsing Pip:\n\n\npip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models\n\n\n\n\nInstalling from source:\n\n\ngit clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install\n\n\n\n\nQuick Start\n\n\nBelow demonstrates how a Gandlf model works.\n\n\nimport keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n# \ninput_data\n represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -\n 1\n# and fake 'generated data' -\n 0, while the generator learns\n# 'generated data' -\n 1.\nmodel.fit(['normal', \ninput_data\n], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal', \ninput_data\n], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input': \ninput_data\n},\n          {'src_gen_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([\ninput_data\n])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')\n\n\n\n\nGuiding Principles\n\n\nIn no particular order:\n\n\n\n\nKeras-esque\n: The APIs should feel familiar for Keras users, with some minor changes.\n\n\nPowerful\n: Models should support a wide variety of GAN architectures.\n\n\nExtensible\n: Models should be easy to modify for different experiments.\n\n\n\n\nIssues Etiquette\n\n\nMore examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the \nexamples\n directory. Just create a pull request for it.\n\n\nContribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).\n\n\nIf you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Home"
        }, 
        {
            "location": "/#generative-adversarial-network-deep-learning-framework", 
            "text": "He that breaks a thing to find out what it is has left the path of wisdom.\n(Tim Peters, Council of Elrond Style Guide)   This is a framework built on top of  Keras  for training  Generative Adversarial Networks .  Because it's built on top of Keras, it has the benefits of being  modular ,  minimal  and  extensible , running on both CPU and GPU using either Tensorflow or Theano.", 
            "title": "Generative Adversarial Network Deep Learning Framework"
        }, 
        {
            "location": "/#installation", 
            "text": "Using Pip:  pip install git+https://github.com/codekansas/gandlf\npip install h5py  # To save and load Keras models  Installing from source:  git clone https://github.com/codekansas/gandlf\ncd gandlf\npip install -r requirements.txt\npython setup.py install", 
            "title": "Installation"
        }, 
        {
            "location": "/#quick-start", 
            "text": "Below demonstrates how a Gandlf model works.  import keras\nimport gandlf\n\ndef build_generator():\n    latent_vec = keras.layers.Input(shape=..., name='latent_vec')\n    output_layer = keras.layers.Dense(...)(latent_vec)\n    return keras.models.Model(input=[latent_vec], output=[output_layer])\n\ndef build_discriminator():\n    data_input = keras.layers.Input(shape=..., name='data_input')\n    output_layer = keras.layers.Dense(..., name='src')(data_input)\n    return keras.models.Model(input=[data_input], output=[output_layer])\n\nmodel = gandlf.Model(generator=build_generator(),\n                     discriminator=build_discriminator())\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy')\n\n# Latent vector is fed data from a random normal distribution.\n#  input_data  represents the real data.\n# 'zeros' and 'ones' represent 'fake' and 'real', respectively.\n# In this case, the discriminator learns 'real data' -  1\n# and fake 'generated data' -  0, while the generator learns\n# 'generated data' -  1.\nmodel.fit(['normal',  input_data ], ['ones', 'zeros'])\n\n# There are many ways to do the same thing, depending on the level\n# of specificity you need (especially when training with auxiliary parts).\n# The above function could be written as any of the following:\nmodel.fit(['normal',  input_data ], {'gen_real': 'ones', 'fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src': 'ones', 'src_fake': 'zeros'})\nmodel.fit({'latent_vec': 'normal', 'data_input':  input_data },\n          {'src_gen_real': '1', 'src_fake': '0'})\n\n# The model provides a function for predicting the discriminator's\n# output on some input data, which is useful for auxiliary classification.\nmodel_predictions = model.predict([ input_data ])\n\n# The model also provides a function for sampling from the generator.\ngenerated_data = model.sample(['normal'], num_samples=10)\n\n# Under the hood, other functions work like their Keras counterparts.\nmodel.save('/save/path')", 
            "title": "Quick Start"
        }, 
        {
            "location": "/#guiding-principles", 
            "text": "In no particular order:   Keras-esque : The APIs should feel familiar for Keras users, with some minor changes.  Powerful : Models should support a wide variety of GAN architectures.  Extensible : Models should be easy to modify for different experiments.", 
            "title": "Guiding Principles"
        }, 
        {
            "location": "/#issues-etiquette", 
            "text": "More examples would be awesome! If you use this for something, create a stand-alone script that can be run and I'll put it in the  examples  directory. Just create a pull request for it.  Contribute code too! Anything that might be interesting and relevant for building GANs. Since this is more task-specific than Keras, there is more room for more experimental layers and ideas (notice that \"dependability\" isn't one of the guiding principles, although it would be good to not have a huge nest of bugs).  If you encounter an error, I would really like to hear about it! But please raise an issue before creating a pull request, to discuss the error. Even better, look around the code to see if you can spot what's going wrong. Try to practice good etiquette, not just for this project, but for open source projects in general; this means making an honest attempt at solving the problem before asking for help.", 
            "title": "Issues Etiquette"
        }, 
        {
            "location": "/background/", 
            "text": "What is a GAN?\n\n\nGAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.\n\n\n\n\nThe GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.\n\n\nGANs were introduced by Ian Goodfellow in an \neponymous paper\n. Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular \nKeras\n library, and therefore supports training in both Theano and Tensorflow.\n\n\nWhy are GANs interesting?\n\n\nGANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).\n\n\nTo understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.\n\n\nWhat is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.\n\n\nResources\n\n\nBelow are some resources for learning more about GANs.\n\n\n\n\n/r/MachineLearning\n\n\nAdversarial Training Facebook Group", 
            "title": "Background"
        }, 
        {
            "location": "/background/#what-is-a-gan", 
            "text": "GAN stands for Generative Adversarial Network. The graphic below provides a scientific illustration of what a GAN does.   The GAN consists of two neural networks: a generator and a discriminator. The discriminator's job is to take in some real data and some fake data and decide which one is which. The generator's job is to generate good enough fake data that the discriminator can't tell the difference.  GANs were introduced by Ian Goodfellow in an  eponymous paper . Since then, many researchers have come up with different architectures and tricks to improve training. Gandlf aims to make these advancements more accessible and provide a platform for experimenting with different tweaks. It is built on top of the popular  Keras  library, and therefore supports training in both Theano and Tensorflow.", 
            "title": "What is a GAN?"
        }, 
        {
            "location": "/background/#why-are-gans-interesting", 
            "text": "GANs are one of the many exciting things to come out of the field of deep learning in the last several years. They have been used to produce some very realistic-looking data samples (check out the Examples tabs).  To understand why GANs are worth thinking about, here is a thought experiment. Imagine you're watching cars, and trying to predict the color of the next car to drive by. You predict that it will be red, but it turns out to be blue. A conventional neural network loss function would punish your guess, even though it is perfectly reasonable, much more reasonable than guessing, for example, that the next car would be a horse. To be safe, you would have to guess some average of all the possible car colors, so that you're not totally wrong, but never totally right, either. With a GAN, however, would be punish you for guessing a horse, but not for guessing the wrong color car; in other words, as long as you guess a reasonable color, the GAN would be happy.  What is going on here? After some training, hopefully the discriminator learns a representation of \"realistic\" data. From this representation, the generator learns to \"trick\" the discriminator. To do this, it just has to produce \"realistic\" samples; it is only punished if the samples it produces don't seem realistic.", 
            "title": "Why are GANs interesting?"
        }, 
        {
            "location": "/background/#resources", 
            "text": "Below are some resources for learning more about GANs.   /r/MachineLearning  Adversarial Training Facebook Group", 
            "title": "Resources"
        }, 
        {
            "location": "/examples/xor/", 
            "text": "This example can be run quickly on a CPU, and is a good demonstration of one of the tricky parts about training GANs. The input data consists of four uniform distributions, centered near \n(-1, -1)\n, \n(1, -1)\n, \n(-1, 1)\n and \n(1, 1)\n. This is illustrated in the figure below, which each of the distributions labeled.\n\n\n\n\nThe model can either be trained in unsupervised mode or supervised mode. In the supervised mode, it acts as an auxiliary classifier GAN, which explicitly says which distribution the generated data should come from.\n\n\nWhen trained in the unsupervised mode, the data tends to cluster in one of the distributions. A potential way to fix this would be to let the GAN look at a whole batch of data, which would let it know that it's clustering too much on one distribution.", 
            "title": "XOR"
        }
    ]
}